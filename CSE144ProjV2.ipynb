{"cells":[{"cell_type":"markdown","metadata":{"id":"8OHfFKB-poPa"},"source":["THINGS TO DO (IN ORDER)\n","- import everything\n","- get the data\n","- analyzze data\n","  - oversample if needed\n","  - clean up anything weird (there shouldn't be much)\n","- split the data (train and test)\n","- training data\n","  - find best parameters for this / different models\n","- test the data\n","- get all results\n","- map results\n","- train CNN\n","- train Transformer\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5428,"status":"ok","timestamp":1718164751098,"user":{"displayName":"Abdullah Riaz","userId":"17825054664034101233"},"user_tz":420},"id":"eGxlArWWXDgM","outputId":"27e76255-9c2a-4da0-b060-c02c8e95378f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'CSE144-G9'...\n","remote: Enumerating objects: 1306, done.\u001b[K\n","remote: Counting objects: 100% (1306/1306), done.\u001b[K\n","remote: Compressing objects: 100% (1300/1300), done.\u001b[K\n","remote: Total 1306 (delta 16), reused 1265 (delta 3), pack-reused 0\u001b[K\n","Receiving objects: 100% (1306/1306), 45.66 MiB | 14.40 MiB/s, done.\n","Resolving deltas: 100% (16/16), done.\n","/content/CSE144-G9\n"]}],"source":["!git clone https://github.com/Samintha-C/CSE144-G9.git\n","%cd CSE144-G9/"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"_25_DshYjhM8","executionInfo":{"status":"ok","timestamp":1718164766050,"user_tz":420,"elapsed":14956,"user":{"displayName":"Abdullah Riaz","userId":"17825054664034101233"}}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import pandas as pd\n","%matplotlib inline\n","%config InlineBackend.figure_format=\"retina\"\n","import numpy as np\n","import random\n","import torch\n","from torch import nn, optim\n","import math\n","from IPython import display\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","from torch.utils.data import TensorDataset\n","import pdb\n","from sklearn.neural_network import MLPClassifier\n","from sklearn import model_selection\n","from sklearn import metrics\n","from collections import Counter\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.model_selection import cross_val_score\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from PIL import Image\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.metrics import r2_score, explained_variance_score, mean_absolute_error\n","from torchvision import transforms, models\n","from PIL import Image\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import os\n","from torch.utils.data import Dataset\n","from PIL import Image, ImageEnhance\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"NcvtOEqkj6qw","executionInfo":{"status":"ok","timestamp":1718164766050,"user_tz":420,"elapsed":16,"user":{"displayName":"Abdullah Riaz","userId":"17825054664034101233"}}},"outputs":[],"source":["#traindf = pd.read_csv(\"_classes.csv\")\n","#traindf.describe()\n","#traindf[\"\"]\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"f1PSZpKSWSpx","executionInfo":{"status":"ok","timestamp":1718164766460,"user_tz":420,"elapsed":425,"user":{"displayName":"Abdullah Riaz","userId":"17825054664034101233"}}},"outputs":[],"source":["# Data loader function to load data into batches\n","\n","class Data(Dataset):\n","    def __init__(self, csv_file, root_dir, transform=None):\n","        \"\"\"\n","        csv_file: Path to the csv file.\n","        root_dir: Directory with all the images.\n","        transform: transform to be applied on a sample. optional\n","        \"\"\"\n","        self.frame = pd.read_csv(csv_file)\n","        self.root_dir = root_dir\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.frame)\n","\n","    def __getitem__(self, idx):\n","        img_name = os.path.join(self.root_dir, self.frame.iloc[idx, 0])\n","        image = Image.open(img_name)\n","        #labels = self.frame.iloc[idx, 1:].to_numpy()\n","        # Convert labels to single class labels (assuming one-hot encoding)\n","        #labels = np.argmax(labels, axis=0)  # Find index of the class with highest probability\n","        #labels = torch.tensor(labels).long()  # Convert to LongTensor for CrossEntropyLoss\n","        labels = self.frame.iloc[idx, 1:].to_numpy()\n","        labels = labels.astype(np.float32)  # Convert labels to float32\n","        labels = torch.tensor(labels).float()  # Convert to FloatTensor for BCEWithLogitsLoss\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, labels\n","\n","transform = transforms.Compose([\n","    transforms.Resize((416, 416)),\n","    transforms.ToTensor(),\n","])\n","\n","# Dataset and DataLoader\n","train_dataset = Data(csv_file='Dataset/train/_classes.csv', root_dir='Dataset/train/', transform=transform)\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n","\n","validation_dataset = Data(csv_file='Dataset/valid/_classes.csv', root_dir='Dataset/valid/', transform=transform)\n","validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=32, shuffle=False)\n","\n","\n","test_dataset = Data('Dataset/test/_classes.csv', root_dir='Dataset/test/', transform=transform)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n"]},{"cell_type":"code","source":["num_classes = 6\n","class Net(nn.Module):\n","    def __init__(self, num_classes=10):\n","        super(Net, self).__init__()\n","        self.layer1 = nn.Sequential(\n","            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # Output: (32, 416, 416)\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),       # Output: (32, 208, 208)\n","            nn.Dropout(0.25))\n","        self.layer2 = nn.Sequential(\n","            nn.Conv2d(32, 64, kernel_size=3),            # Output: (64, 206, 206)\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2),                             # Output: (64, 103, 103)\n","            nn.Dropout(0.25))\n","        self.layer3 = nn.Sequential(\n","            nn.Conv2d(64, 128, kernel_size=3),           # Output: (128, 101, 101)\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2),                             # Output: (128, 50, 50)\n","            nn.Dropout(0.25))\n","\n","        self.fc1 = nn.Linear(128 * 50 * 50, 500)         # Adjusted input size\n","        self.fc2 = nn.Linear(500, num_classes)\n","\n","    def forward(self, x):\n","        out = self.layer1(x)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = out.view(out.size(0), -1)\n","        out = self.fc1(out)\n","        out = self.fc2(out)\n","        return out\n"],"metadata":{"id":"eacBqI1oHQUg","executionInfo":{"status":"ok","timestamp":1718164766460,"user_tz":420,"elapsed":3,"user":{"displayName":"Abdullah Riaz","userId":"17825054664034101233"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class OptimizedCNN(nn.Module):\n","    def __init__(self, num_classes=6):\n","        super(OptimizedCNN, self).__init__()\n","\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n","        self.bn1 = nn.BatchNorm2d(32)\n","        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n","        self.bn2 = nn.BatchNorm2d(64)\n","        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n","        self.bn3 = nn.BatchNorm2d(128)\n","        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n","        self.bn4 = nn.BatchNorm2d(256)\n","        self.conv5 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)\n","        self.bn5 = nn.BatchNorm2d(512)\n","        self.conv6 = nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, stride=1, padding=1)\n","        self.bn6 = nn.BatchNorm2d(1024)\n","\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n","\n","        self.dropout = nn.Dropout(p=0.5)\n","\n","        self.fc1 = nn.Linear(1024 * 6 * 6, 4096)\n","        self.fc2 = nn.Linear(4096, 1024)\n","        self.fc3 = nn.Linear(1024, num_classes)\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n","        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n","        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n","        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n","        x = self.pool(F.relu(self.bn5(self.conv5(x))))\n","        x = self.pool(F.relu(self.bn6(self.conv6(x))))\n","\n","        x = x.view(-1, 1024 * 6 * 6)\n","\n","        x = self.dropout(F.relu(self.fc1(x)))\n","        x = self.dropout(F.relu(self.fc2(x)))\n","        x = self.fc3(x)\n","\n","        return x\n","\n","model = OptimizedCNN(num_classes=6)\n"],"metadata":{"id":"qx234DOpNI6_","executionInfo":{"status":"ok","timestamp":1718164768628,"user_tz":420,"elapsed":2170,"user":{"displayName":"Abdullah Riaz","userId":"17825054664034101233"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["'''EPOCHS = 3\n","\n","def train():\n","    model.train()\n","    for epoch in range(EPOCHS):\n","        #Note: 242 batches in epoch\n","        for i, data in enumerate(train_loader, 0):\n","            # get the inputs; data is a list of [inputs, labels]\n","            inputs, labels = data\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            if i % 100 == 0:\n","                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                    epoch, i * len(data), len(train_loader.dataset),\n","                    100. * i / len(train_loader), loss.item()))\n","\n","\n","    print('Done Training')\n","train()'''\n","x=1"],"metadata":{"id":"BNfSfwsyF5AP","executionInfo":{"status":"ok","timestamp":1718164768629,"user_tz":420,"elapsed":10,"user":{"displayName":"Abdullah Riaz","userId":"17825054664034101233"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision import transforms, datasets\n","\n","class CNNModel(nn.Module):\n","    def __init__(self, num_classes=6):\n","        super(CNNModel, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n","        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n","        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n","        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n","        self.conv5 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n","        self.fc1 = nn.Linear(512 * 13 * 13, 1024)\n","        self.fc2 = nn.Linear(1024, 512)\n","        self.fc3 = nn.Linear(512, num_classes)\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = self.pool(F.relu(self.conv3(x)))\n","        x = self.pool(F.relu(self.conv4(x)))\n","        x = self.pool(F.relu(self.conv5(x)))\n","        x = x.view(-1, 512 * 13 * 13)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        x = torch.sigmoid(x)\n","        return x\n","\n","# Create a model instance\n","model = CNNModel(num_classes=6)\n","\n","# Print model architecture\n","print(model)\n","\n","# Define a loss function and optimizer\n","criterion = nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# Example usage with dummy data\n","# Assuming `train_loader` is a DataLoader object that provides the training data\n","# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","\n","# Training loop (simplified example)\n","def train_model(model, criterion, optimizer, train_loader, num_epochs=10):\n","    for epoch in range(num_epochs):\n","        running_loss = 0.0\n","        for i in train_loader:\n","            # Zero the parameter gradients\n","            optimizer.zero_grad()\n","            images = i[0]\n","            # Concatenate labels into a single tensor\n","            labels = torch.cat(i[1:], dim=1)\n","            # Forward pass\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","\n","            # Backward pass and optimization\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","\n","        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n","\n","# Example transform for the dataset\n","transform = transforms.Compose([\n","    transforms.Resize((416, 416)),\n","    transforms.ToTensor(),\n","])\n","\n","# Example dataset and dataloader\n","# Assuming `data_dir` is the directory with your dataset\n","\n","# Train the model (this will need actual data in the 'data/train' directory)\n","train_model(model, criterion, optimizer, train_loader, num_epochs=10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AadwWyAzxbvZ","outputId":"2e4fe0b1-751c-48bb-b0e2-6f63af0edf63"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CNNModel(\n","  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv5): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (fc1): Linear(in_features=86528, out_features=1024, bias=True)\n","  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n","  (fc3): Linear(in_features=512, out_features=6, bias=True)\n",")\n"]}]},{"cell_type":"code","source":["import torch.optim as optim\n","from IPython import display\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","def evaluate_model(model, dataloader, is_test=False):\n","    # Evaluation\n","    model.eval()\n","\n","    with torch.no_grad():\n","        total_correct = 0\n","        total_loss = 0\n","\n","        criterion = torch.nn.CrossEntropyLoss(reduction='none')\n","\n","        for data, targets in dataloader:\n","            data = data.to(device)\n","            targets = [target.to(device) for target in targets]\n","\n","            outputs = model(data)\n","            losses = [torch.sum(criterion(output, target)).item() for output, target in zip(outputs, targets)]\n","            total_loss += sum(losses)\n","\n","            correct = 0\n","            for output, target in zip(outputs, targets):\n","                pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n","                correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n","            total_correct += correct\n","\n","        total_loss /= len(dataloader.dataset)\n","        accuracy = 100. * total_correct / (len(dataloader.dataset) * len(targets))\n","        print('\\n{} set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","            \"Test\" if is_test else \"Validation\",\n","            total_loss, total_correct, len(dataloader.dataset) * len(targets),\n","            accuracy))\n","\n","    # Set model back to training mode\n","    model.train()\n","\n","def train_model(model, train_loader, optimizer, epochs=30):\n","    criterion = nn.CrossEntropyLoss()\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        for data, targets in train_loader:\n","            data = data.to(device)\n","            # Ensure targets are not empty and have the correct shape\n","            if not all(target.numel() > 0 for target in targets):  # Check if all target tensors have elements\n","                print(\"Warning: Empty target batch encountered. Skipping this batch.\")\n","                continue  # Skip this batch if any target is empty\n","\n","            targets = [target.to(device) for target in targets]\n","\n","            optimizer.zero_grad()\n","            outputs = model(data)\n","            losses = [criterion(output, target) for output, target in zip(outputs, targets)]\n","            loss = sum(losses)\n","\n","            print(\"[EPOCH]: %i, [LOSS]: %.6f\" % (epoch, loss.item()))\n","            display.clear_output(wait=True)\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","# Example usage:\n","# Assuming train_loader and val_loader are defined and provide a list of targets for each data point\n","\n","learning_rate = 0.01\n","epochs = 10\n","\n","model = OptimizedCNN(num_classes_list).to(device)\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training\n","train_model(model, train_loader, optimizer, epochs)\n","\n","# Evaluation\n","evaluate_model(model, validation_loader)\n","evaluate_model(model, test_loader, is_test=True)"],"metadata":{"id":"ekOEYTscx6t7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''def evaluate_model(model, dataloader, is_test=False):\n","  #Evaluation\n","\n","  # Set model to evaluation mode\n","  model.eval()\n","\n","  with torch.no_grad():\n","    correct = 0\n","    loss = 0\n","\n","    criterion = torch.nn.CrossEntropyLoss(reduction='none')\n","\n","    for data, target in dataloader:\n","        data, target = data.to(device), target.to(device)\n","        outputs = model(data)\n","\n","        loss += torch.sum(criterion(outputs, target)).item()\n","\n","        pred = outputs.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n","        correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n","\n","    loss /= len(dataloader.dataset)\n","    accuracy = 100. * correct / len(dataloader.dataset)\n","    print('\\n{} set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        \"Test\" if is_test else \"Validation\",\n","        loss, correct, len(dataloader.dataset),\n","        accuracy))\n","  # Set model back to training mode\n","  model.train()\n","\n","###### Your code starts here. ######\n","\n","def train_model(M):\n","    criterion = nn.CrossEntropyLoss()\n","    for epoch in range(epochs):\n","        for input, label in train_loader:\n","            input, label = input.to(device), label.to(device)\n","            pred_label = M(input)\n","            optimizer.zero_grad()\n","            loss = criterion(pred_label, label)\n","            print(\"[EPOCH]: %i, [LOSS or MSE]: %.6f\" % (epoch, loss.item()))\n","            display.clear_output(wait=True)\n","            loss.backward()\n","            optimizer.step()\n","\n","\n","###### Your code end here. ######\n","learning_rate = 0.01\n","epochs = 30'''"],"metadata":{"id":"AluB1voJK3dp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)  # Using Adam optimizer with learning rate 0.001\n","\n","train_model(model)\n","\n"],"metadata":{"id":"wgekOetTLQ06"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["evaluate_model(model, validation_loader, is_test=True)"],"metadata":{"id":"bmW8_beNLfrd"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_OODO6f_WSpx"},"outputs":[],"source":["for a_type in _ :\n","  count = a_type[a_type]\n","  if count < 300:\n","    count_to_change = (0.18 * count) / 3\n","  else:\n","    count_to_change = (0.05 * count) / 3\n","  change_im_df = traindf[a_type].sample(frac=count_to_change)\n","  for image in change_im_df :\n","    sheared_im = image.clone().shear('Black', 20, 30) # should print out and check if this is too awkward..\n","    pixeliated_im = image.clone().resize(300, 226) # should print out and check if this is too grainy..\n","    darker_im = ImageEnhance.Brightness(image.clone()).enhance() # should print out and check if this is too dark..\n","    altered_df = pd.DataFrame({a_type:[sheared_im, pixeliated_im, darker_im]})\n","    traindf.append(altered_df, ignore_index=True) # might be more complicated than this ...\n"]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}